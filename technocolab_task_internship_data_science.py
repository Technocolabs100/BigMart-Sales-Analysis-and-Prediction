# -*- coding: utf-8 -*-
"""Technocolab_task_internship_data_science

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1akhCUbIjTZsj1KryOLFAD6MQiYYUrYcF
"""

!pip install cartopy
import cartopy

!apt-get -qq install -y graphviz && pip install pydot
import pydot

!apt-get -qq install -y libarchive-dev && pip install -U libarchive
import libarchive

!apt-get -qq install -y libfluidsynth1

!pip install matplotlib-venn

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns 
# %matplotlib inline

from sklearn import preprocessing
from sklearn.model_selection import train_test_split
import sklearn
from sklearn import linear_model
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
# from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score, KFold
from sklearn import metrics 
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error

data=pd.read_csv("/content/Train.csv")

data.head()

data.info()

data.shape

data.isnull().sum()

data['Outlet_Size'].fillna(data['Outlet_Size'].mode()[0],inplace=True)

data['Item_Weight'].fillna(data['Item_Weight'].mean(),inplace=True)

data.isnull().sum()

data["Item_Fat_Content"].unique()

data['Item_Fat_Content'].replace(['low fat',  'LF','reg'],
                        ['Low Fat', 'Low Fat','Regular'], inplace=True)



data['Item_Fat_Content'].unique()

fornum = preprocessing.LabelEncoder()
data['Item_Fat_Content']= fornum.fit_transform(data['Item_Fat_Content'])
data['Item_Fat_Content'].unique()

data["Item_Type"].unique()

data['Item_Type']= fornum.fit_transform(data['Item_Type'])
data['Item_Type'].unique()

data["Outlet_Identifier"].unique()

data['Outlet_Identifier']= fornum.fit_transform(data['Outlet_Identifier'])
data['Outlet_Identifier'].unique()

data["Outlet_Size"].unique()

data['Outlet_Size']= fornum.fit_transform(data['Outlet_Size'])
data['Outlet_Size'].unique()

data["Outlet_Location_Type"].unique()

data['Outlet_Location_Type']= fornum.fit_transform(data['Outlet_Location_Type'])
data['Outlet_Location_Type'].unique()

data["Outlet_Type"].unique()

data['Outlet_Type']= fornum.fit_transform(data['Outlet_Type'])
data['Outlet_Type'].unique()

data["Item_Identifier"].unique()

data['Item_Identifier']= fornum.fit_transform(data['Item_Identifier'])
data['Item_Identifier'].unique()

data.info()

for col in data.columns:
    fig = plt.figure(figsize=(2,2))
    plt.title(f"{col}")
    data[col].hist()

plt.subplots(figsize=(10,5))
sns.heatmap(data.corr(),annot=True)

#Showing the uncorrelated features 
def correlation(data,threshold):
  col_corr=set()
  corr_matrix=data.corr()
  for i in range(len(corr_matrix.columns)):
   for j in range(i):
      if abs(corr_matrix.iloc[i,j])>threshold:
       colname=corr_matrix.columns[i]
       col_corr.add(colname)
  return col_corr

costing_features =correlation(data,0.015)
costing_features

data.drop(['Item_Identifier'],axis=1,inplace=True)

plt.subplots(figsize=(17,7))
sns.boxplot(data=data,palette='rainbow',orient='h')

plt.rcParams["figure.figsize"] = [14.50, 7.50]
plt.subplot(231)
plt.boxplot(data['Item_Outlet_Sales'],labels=' ');
plt.xlabel('Item_Outlet_Sales')
plt.subplot(232)
plt.boxplot(data['Outlet_Type'],labels=' ');
plt.xlabel('Outlet_Type')
plt.subplot(233)
plt.boxplot(data['Item_Visibility'],labels=' ');
plt.xlabel('Item_Visibility')

#Let's try get ride of outliers!
for x in ['Item_Outlet_Sales','Item_Visibility']:
    q75,q25 = np.percentile(data.loc[:,x],[75,25])
    intr_qr = q75-q25
 
    max = q75+(1.5*intr_qr)
    min = q25-(1.5*intr_qr)
  
    data.loc[data[x] > max,x] = max

plt.rcParams["figure.figsize"] = [14.50, 7.50]
plt.subplot(231)
plt.boxplot(data['Item_Outlet_Sales'],labels=' ');
plt.xlabel('Item_Outlet_Sales')
plt.subplot(232)
plt.boxplot(data['Outlet_Type'],labels=' ');
plt.xlabel('Outlet_Type')
plt.subplot(233)
plt.boxplot(data['Item_Visibility'],labels=' ');
plt.xlabel('Item_Visibility')

sns.boxplot(data=data,palette='rainbow',orient='h')

data.duplicated().sum()

data.drop_duplicates(inplace=True)
data.duplicated().sum()

sns.pairplot(data)

sns.pairplot(
    data,
    x_vars=['Item_Fat_Content',
 'Item_MRP',
 'Item_Type',
 'Item_Visibility',
 'Item_Weight',
 'Outlet_Establishment_Year',
 'Outlet_Identifier',
 'Outlet_Location_Type',
 'Outlet_Size',
 'Outlet_Type'],
    y_vars=["Item_Outlet_Sales"],hue='Item_Outlet_Sales',
    palette='coolwarm'
)

import plotly.express as ex
ex.pie(data,names='Outlet_Type',hole=0.20,width=500,height=500)

ex.pie(data,names='Outlet_Size',hole=0.20,width=500,height=500)

ex.pie(data,names='Outlet_Location_Type',hole=0.20,width=500,height=500)

plt.subplots(figsize=(5,5))
sns.distplot(data['Item_Outlet_Sales'])

data.describe()

from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
 
dataset = load_iris()
object= StandardScaler()
 
# Splitting the independent and dependent variables
i_data = dataset.data
response = dataset.target
 
# standardization 
scale = object.fit_transform(i_data) 
print(scale)

object = StandardScaler()
object.fit_transform(data)

X=data.drop(["Item_Outlet_Sales"],axis=1)
Y=data["Item_Outlet_Sales"]

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.24, random_state = 35)

"""**Linear Regression**"""

regressor = LinearRegression()
regressor.fit(X_train, y_train)
y_pred = regressor.predict(X_test)
print("MSE:",metrics.mean_squared_error(y_pred,y_test))
print("MAE:",metrics.mean_absolute_error(y_pred,y_test))
print("RMSE:",np.sqrt(metrics.mean_squared_error(y_pred,y_test)))
print("r2_score:",metrics.r2_score(y_pred,y_test))

regressor.score(X_train,y_train)

print(regressor.score(X_test, y_test))

"""**Regularized Linear Regression**"""

ridge=Ridge(alpha=3)
ridge.fit(X_train,y_train)
print ("Ridge model:", (ridge.coef_))

print(ridge.score(X_train, y_train))
print(ridge.score(X_test, y_test))

lasso = Lasso(alpha=0.1)
lasso.fit(X_train,y_train)
print ("Lasso model:", (lasso.coef_))

print(lasso.score(X_train, y_train))
print(lasso.score(X_test, y_test))

"""**Random Forest Regressor**"""

RFR_model=RandomForestRegressor(n_estimators=100,random_state=1)
RFR_model.fit(X_train, y_train)

RFR_model_prediction = RFR_model.predict(X_test)

print("MSE:",mean_squared_error(RFR_model_prediction,y_test))
print("MAE:",metrics.mean_absolute_error(RFR_model_prediction,y_test))
print("RMSE:",np.sqrt(metrics.mean_squared_error(RFR_model_prediction,y_test)))
print("r2_score:",metrics.r2_score(RFR_model_prediction,y_test))

RFR_model.score(X_test, y_test)

"""**XGB model**"""

#  create XGB model
xgb_reg = xgb.XGBRegressor(max_depth=3, n_estimators=100, n_jobs=2,
                           objectvie='reg:squarederror', booster='gbtree',
                           random_state=42, learning_rate=0.05)

# fit the model to the data
xgb_reg.fit(X_train, y_train)

xgb_reg_prediction = xgb_reg.predict(X_test)
print("MSE:",mean_squared_error(xgb_reg_prediction,y_test))
print("MAE:",metrics.mean_absolute_error(xgb_reg_prediction,y_test))
print("RMSE:",np.sqrt(metrics.mean_squared_error(xgb_reg_prediction,y_test)))
print("r2_score:",metrics.r2_score(xgb_reg_prediction,y_test))

xgb_reg.score(X_test, y_test)

"""**Saving The Final Model**"""

#Saving the model Random Forest Regressor
import pickle
filename = '/content/Test.csv'
pickle.dump(RFR_model, open(filename, 'wb'))
# loading the saved model
loaded_model = pickle.load(open('/content/Test.csv', 'rb'))